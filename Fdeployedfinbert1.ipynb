{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Final Deploment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNM5dVY1R3vq+iSiB0f4D/U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunsawhney8/Projects/blob/main/Fdeployedfinbert1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voR07Afz9g3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42719359-8434-4e59-820a-1f766d82901b"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (0.87.0)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.18.2)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.2.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.5)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.7.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.1.18)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit) (4.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit) (3.7.4.3)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (4.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (6.3.0)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.12.3)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: importlib-metadata<5 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.6.4)\n",
            "Requirement already satisfied: ipython<8.0,>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (7.27.0)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.5.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.0.20)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.2.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.11.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWW_6v-fKVYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1772ce8f-6f8f-4576-d018-1a5e5fb87ac0"
      },
      "source": [
        "!pip install pyngrok==4.1.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15984 sha256=7248fb4c7831892497a1814b5766c3fcbe551257b8bf98baaccde80cd0c3a5ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9mDL2XtMJll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8805a31e-4224-4c22-d4b2-ad7f70aa1265"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.9.2-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 41.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 57.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.12 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsZCX-1K-jcE",
        "outputId": "2604a8c3-2f7b-48ea-fbc1-ef42690fc66a"
      },
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# Project: Sentiment Analysis \n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import nltk\n",
        "import requests\n",
        "import os\n",
        "import streamlit as st\n",
        "import altair as alt\n",
        "\n",
        "\n",
        "######## output ###############################################################\n",
        "\n",
        "#company_nse_2.to_csv(\"Reliance_Nse_data2.csv\",encoding = \"utf-8\")\n",
        "\n",
        "# b) Load dataset\n",
        "import pandas as pd\n",
        "path='/content/Reliance_Final_Ndtv_data4.xlsx'\n",
        "\n",
        "data=pd.read_excel(path)\n",
        "\n",
        "data=data[::-1].reset_index()\n",
        "data.drop(columns=['index'],axis=True,inplace=True)\n",
        "\n",
        "path='/content/Reliance_Nse_data2.csv'\n",
        "\n",
        "data1=pd.read_csv(path)\n",
        "\n",
        "# 2. Summarize Data\n",
        "# a) Descriptive statistics\n",
        "# b) Data visualizations\n",
        "\n",
        "data.drop(columns=['Media','Article','Keywords'],inplace=True,axis=1)\n",
        "\n",
        "# 3. Prepare Data\n",
        "# a) Data Cleaning\n",
        "# b) Feature Selection\n",
        "# c) Data Transforms\n",
        "def clean_data(data):\n",
        "    \n",
        "    list1=[]\n",
        "    list1=data['Title']\n",
        "    import re\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    \n",
        "    for i in range(len(list1)):               \n",
        "        tokens = list1[i].split()\n",
        "        # prepare regex for char filtering\n",
        "        re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "        # remove punctuation from each word\n",
        "        tokens = [re_punc.sub('', w) for w in tokens]\n",
        "        # remove remaining tokens that are not alphabetic\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "        # filter out stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        # filter out short tokens\n",
        "        tokens = [word for word in tokens if len(word) > 1]\n",
        "        list1[i] = ' '.join(tokens)\n",
        "    data['Title']=list1\n",
        "    return data\n",
        "\n",
        "    \n",
        "#del list1\n",
        "\n",
        "# 4. Evaluate Algorithms\n",
        "# a) Split-out validation dataset- train_test_split\n",
        "# b) Test options and evaluation metric- Confusion matrix, accuracy, precision,recall, f measure\n",
        "# c) Spot Check Algorithms- Unsupervised [Textblob, Vader, LM Dictionary,FINBERT], Supervised-[Bert]\n",
        "# d) Compare Algorithms\n",
        "\n",
        "# =============================================================================\n",
        "# \"\"\" MODEL:**\n",
        "# 1. FINBERT\n",
        "# \"\"\"\n",
        "# =============================================================================\n",
        "#!pip install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "def model_sentiment(data):\n",
        "    \n",
        "    classifier = pipeline('sentiment-analysis', model=\"ProsusAI/finbert\")\n",
        "    list3=[]\n",
        "    \n",
        "    for i in range(len(data['Title'])):\n",
        "        a=(classifier(data['Title'][i]))\n",
        "        a=a[0]\n",
        "        a=list(a.values())[0]\n",
        "        label_f={'positive':1,'negative':2,'neutral':0}\n",
        "        list3.append(label_f[a])\n",
        "    \n",
        "    data['1day_sentiment']=list3\n",
        "    del list3\n",
        "    return (data)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# \n",
        "# \"\"\"**VISUALIZATION:**\n",
        "# 1. PIE Chart\n",
        "# 2. WORD Cloud\n",
        "# \"\"\"\n",
        "def pie_chart(data):\n",
        "    import matplotlib.pyplot as plt\n",
        "    labels = ['Neutral', 'Positive','Negative']\n",
        "    sizes = [data['1day_sentiment'].value_counts()[0],data['1day_sentiment'].value_counts()[1],data['1day_sentiment'].value_counts()[2]]\n",
        "    # Plot\n",
        "    plt.title('News Classification based on sentiment Analysis')\n",
        "    p1=plt.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "    plt.axis('equal')\n",
        "    return st.pyplot()\n",
        "\n",
        "def word_cloud(data):\n",
        "        \n",
        "    pos=[]\n",
        "    neg=[]\n",
        "    neutral=[]\n",
        "             \n",
        "             # Data Divide into segments\n",
        "             \n",
        "    for i in range(len(data)):\n",
        "        if data['1day_sentiment'][i]=='negative':\n",
        "            pos.append(data['Title'][i])\n",
        "        elif data['1day_sentiment'][i]=='positive':\n",
        "            neg.append(data['Title'][i])\n",
        "        else:\n",
        "            neutral.append(data['Title'][i])\n",
        "        \n",
        "    \n",
        "    \n",
        "    #             \n",
        "    from collections import Counter\n",
        "    vocab=Counter()\n",
        "             \n",
        "    #             \n",
        "    vocab_pos=[]\n",
        "    #             \n",
        "    for i in range(len(pos)):\n",
        "        tokens=pos[i].split()\n",
        "        for j in tokens:\n",
        "            vocab_pos.append(j)\n",
        "    #             \n",
        "    vocab_neg=[]\n",
        "    #             \n",
        "    for i in range(len(neg)):\n",
        "        tokens=neg[i].split()\n",
        "        for j in tokens:\n",
        "            vocab_neg.append(j)\n",
        "    #             \n",
        "    vocab_neutral=[]\n",
        "    #             \n",
        "    for i in range(len(neutral)):\n",
        "        tokens=neutral[i].split()\n",
        "        for j in tokens:\n",
        "            vocab_neutral.append(j)\n",
        "    #             \n",
        "    #             \n",
        "    vocab.update(vocab_pos)\n",
        "    vocab.update(vocab_neutral)\n",
        "          \n",
        "    vocab.update(vocab_neg)          \n",
        "    vocab.most_common(50)\n",
        "    #\n",
        "                 \n",
        "    from wordcloud import WordCloud, STOPWORDS\n",
        "    #             \n",
        "    other_stopwords_to_remove = ['\\\\n', 'n', '\\\\', '>', 'nLines', 'nI',\"n'\", \"hi\"]\n",
        "    STOPWORDS = STOPWORDS.union(set(other_stopwords_to_remove))\n",
        "    stopwords = set(STOPWORDS)\n",
        "    text = str(vocab)\n",
        "    wordcloud = WordCloud(width = 1800, height = 1800, background_color ='white', \n",
        "                             max_words=200, stopwords = stopwords, min_font_size = 10).generate(text)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    return st.pyplot()\n",
        "\n",
        "# \"\"\"#5. Business Problem Analysis\n",
        "#     A. Sentiment Meter\n",
        "#     B. Price Movement analysis based on Sentiment meter\n",
        "# \"\"\"\n",
        "# \n",
        "# \n",
        "# =============================================================================\n",
        "\n",
        "def business(data,data1):\n",
        "    # Groupby based on dates as several news articles are published on a particular day\n",
        "\n",
        "    aggregation_functions = {'Title': 'first', '1day_sentiment': 'max'}\n",
        "    data = data.groupby(data['Date']).aggregate(aggregation_functions)\n",
        "    data.reset_index( inplace=True)\n",
        "    data['Date']=pd.to_datetime(data['Date']).dt.date\n",
        "    data['1day_sentiment']=data['1day_sentiment'].astype('float')\n",
        "    \n",
        "    # Calculating Exponential Weighted average for 3,7,15 days sentiments\n",
        "\n",
        "    data['3day_sentiment'] = round(data['1day_sentiment'].ewm(span=3).mean())\n",
        "    data['7day_sentiment'] = round(data['1day_sentiment'].ewm(span=7).mean())\n",
        "    data['15day_sentiment'] = round(data['1day_sentiment'].ewm(span=15).mean())\n",
        "    data.head()\n",
        "    \n",
        "    # Converting date time format\n",
        "    data1['Date']=pd.to_datetime(data1['Date']).dt.date\n",
        "    \n",
        "    # Calculating percentage change w.r.t closing price\n",
        "\n",
        "    data1['perc_change1'] =data1['Close'].pct_change(periods=1)*100\n",
        "    \n",
        "    #data1['perc_change3'] =data1['Close'].pct_change(periods=2)*100\n",
        "    #data1['perc_change7'] =data1['Close'].pct_change(periods=6)*100\n",
        "        \n",
        "    #--\n",
        "    #----\n",
        "    data2=data.drop(columns=['Title'],axis=1)\n",
        "    \n",
        "    # Imputation of perc_change\n",
        "\n",
        "    data2[\"perc_change1\"]=[np.nan for i in range(len(data))]\n",
        "    \n",
        "    label={0: 'Neutral',1:'Positive',2:'Negative'}\n",
        "    data2['1day_sentiment']=[label[data2['1day_sentiment'][i]]for i in range(len(data2))]\n",
        "    data2['3day_sentiment']=[label[data2['3day_sentiment'][i]]for i in range(len(data2))]\n",
        "    data2['7day_sentiment']=[label[data2['7day_sentiment'][i]]for i in range(len(data2))]\n",
        "    data2['15day_sentiment']=[label[data2['15day_sentiment'][i]]for i in range(len(data2))]\n",
        "    \n",
        "    \n",
        "    for i in range(len(data2)):\n",
        "      for j in range(len(data1)):\n",
        "        if data['Date'][i]==data1['Date'][j]:\n",
        "          data2['perc_change1'][i]=data1['perc_change1'][j+1]\n",
        "      count=0\n",
        "      if type(data2['perc_change1'][i]) and pd.isna(data2['perc_change1'][i]):\n",
        "        for j in range(len(data1)):\n",
        "          if data2['Date'][i]>data1['Date'][j]:\n",
        "            count+=1\n",
        "        data2['perc_change1'][i]=data1['perc_change1'][count]\n",
        "    \n",
        "    data2['perc_change1'][0]=np.nan\n",
        "    \n",
        "    data2['Close']=[np.nan for i in range(len(data))]\n",
        "    \n",
        "    for i in range(len(data2)):\n",
        "      for j in range(len(data1)):\n",
        "        if data['Date'][i]==data1['Date'][j]:\n",
        "          data2['Close'][i]=data1['Close'][j+1]\n",
        "      count=0\n",
        "      if type(data2['Close'][i]) and pd.isna(data2['Close'][i]):\n",
        "        for j in range(len(data1)):\n",
        "          if data2['Date'][i]>data1['Date'][j]:\n",
        "            count+=1\n",
        "        data2['Close'][i]=data1['Close'][count]\n",
        "\n",
        "    # To Study how many investment oppurtunities were profitable, loss making\n",
        "    data4=pd.DataFrame(columns=[\"Description\",\"Total Opportunities\",\"Profit(%)\",\"Loss(%)\",\"No Profit No Loss(%)\"])\n",
        "    desc=[]\n",
        "    total1=[]\n",
        "    profit1=[]\n",
        "    loss1=[]\n",
        "    npnl1=[]\n",
        "    col=['1day_sentiment', '3day_sentiment', '7day_sentiment','15day_sentiment']\n",
        "    for j in col:\n",
        "      profit=0\n",
        "      loss=0\n",
        "      npnl=0\n",
        "      for i in range(len(data2)):\n",
        "          if data2[j][i]=='Negative' and  data2['perc_change1'][i]<0:\n",
        "            profit+=1\n",
        "          elif data2[j][i]=='Negative' and  data2['perc_change1'][i]>0:\n",
        "            loss+=1\n",
        "            \n",
        "          if data2[j][i]=='Positive' and  data2['perc_change1'][i]>0:\n",
        "            profit+=1\n",
        "          elif data2[j][i]=='Positive' and  data2['perc_change1'][i]<0:\n",
        "            loss+=1\n",
        "          if data2[j][i]=='Neutral':\n",
        "            npnl+=1\n",
        "      total= profit + loss +npnl\n",
        "      desc.append(j)\n",
        "      total1.append(total)\n",
        "      profit1.append(profit/total*100)\n",
        "      loss1.append(loss/total*100)\n",
        "      npnl1.append(npnl/total*100)\n",
        "    \n",
        "    data4[\"Description\"]=[\"Rolling 1 day-News Day Event\",\"Rolling 3 day-News Day Event\",\"Rolling 7 day-News Day Event \",\"Rolling 15 day-News Day Event\"]\n",
        "    data4[\"Total Opportunities\"]=total\n",
        "    data4['Profit(%)']=profit1\n",
        "    data4['Loss(%)']=loss1\n",
        "    data4['No Profit No Loss(%)']=npnl1\n",
        "    print(\"Back Testing News Based Investment Oppurtunities\")\n",
        "    print(\"Square off trade within  next trading day \")\n",
        "    #display(data4)\n",
        "    \n",
        "    data_1=data.drop(columns=[\"Title\"],axis=1)\n",
        "    data_f1=data_1.iloc[len(data_1)-1,]\n",
        "    label={0.0: 'Neutral',1.0:'Positive',2.0:'Negative'}\n",
        "    f_data=pd.DataFrame(columns=[\"Description\", \"Sentiment\"])\n",
        "    f_data[\"Description\"]=[\"Rolling 1 day-News Day Event\",\"Rolling 3 day-News Day Event\",\"Rolling 7 day-News Day Event \",\"Rolling 15 day-News Day Event\"]\n",
        "    f_data[\"Sentiment\"]=[label[data_f1[i]] for i in range(1,len(data_f1),1)]\n",
        "    \n",
        "    print(\"Current News Based Sentiment Report\")\n",
        "    print(\"Last News Day Recoded : \",data_f1[0])\n",
        "    del data_1\n",
        "    #from IPython.display import display\n",
        "    #display(f_data)\n",
        "    return f_data,data4,data_f1\n",
        "\n",
        "\n",
        "# 5. Improve Accuracy\n",
        "# a) Algorithm Tuning\n",
        "# b) Ensembles\n",
        "\n",
        "# 6. Finalize Model\n",
        "# a) Predictions on validation dataset\n",
        "# b) Create standalone model on entire training dataset\n",
        "# c) Save model for later use\n",
        "\n",
        "data=clean_data(data)\n",
        "data=model_sentiment(data)\n",
        "\n",
        "f_data,data4,data_f1= business(data, data1)\n",
        "\n",
        "from PIL import Image\n",
        "image = Image.open('/content/NLP.PNG')\n",
        "st.image(image, use_column_width=True)\n",
        "st.write(\"\"\"\n",
        "# NLP For Finance\n",
        "***This app allows user to perform sentiment analyis for any BSE Sensex 30 company and provides \n",
        "user useful insights related to what kind of news flow is for a particular company.\n",
        "To make user understand about making sound financial decisions. We cross-validate sentiment\n",
        "analysis results with the stock price movement. ***\n",
        "\"\"\")\n",
        "\n",
        "st.write('News Classification based on sentiment Analysis')\n",
        "#st.write(pie_chart(data))\n",
        "st.write('News based word cloud')\n",
        "#st.write(word_cloud(data))\n",
        "st.write(\"Current News Based Sentiment Report\")\n",
        "st.write(\"Last News Day Recoded : \",data_f1[0])\n",
        "st.write(f_data)\n",
        "st.write(\"Back Testing News Based Investment Oppurtunities\")\n",
        "st.write(\"Square off trade within  next trading day \")\n",
        "st.write(data4)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry1HB8xmkwt3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji6Jkycu-2_H",
        "outputId": "7dc1e5ee-1d80-4abb-8daa-e0cbc7ac3002"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\t Reliance_Final_Ndtv_data4.xlsx  sample_data\n",
            "NLP.PNG  Reliance_Nse_data2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWhL5XRR_EM5",
        "outputId": "6a848e10-9e96-4b09-f6aa-68d7877f850f"
      },
      "source": [
        "!ngrok authtoken ./ngrok authtoken 1xHDQ0ZVGhTEWf2WvFJWvwgCW9U_7MWDgdeCydvLzgnNvBSa4"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME:\n",
            "   authtoken - save authtoken to configuration file\n",
            "\n",
            "USAGE:\n",
            "   ngrok authtoken [command options] [arguments...]\n",
            "\n",
            "DESCRIPTION:\n",
            "   The authtoken command modifies your configuration file to include\n",
            "   the specified authtoken. By default, this configuration file is located\n",
            "   at $HOME/.ngrok2/ngrok.yml\n",
            "\n",
            "   The ngrok.com service requires that you sign up for an account to use\n",
            "   many advanced service features. In order to associate your client with\n",
            "   an account, it must pass a secret token to the ngrok.com service when it\n",
            "   starts up. Instead of passing this authtoken on every invocation, you may\n",
            "   use this command to save it into your configuration file so that your\n",
            "   client always authenticates you properly.\n",
            "\n",
            "EXAMPLE:\n",
            "    ngrok authtoken BDZIXnhJt2HNWLXyQ5PM_qCaBq0W2sNFcCa0rfTZd\n",
            "\n",
            "OPTIONS:\n",
            "   --config \t\tsave in this config file, default: ~/.ngrok2/ngrok.yml\n",
            "   --log \"false\"\tpath to log file, 'stdout', 'stderr' or 'false'\n",
            "   --log-format \"term\"\tlog record format: 'term', 'logfmt', 'json'\n",
            "   --log-level \"info\"\tlogging level\n",
            "\n",
            "ERROR:  You must pass a single argument, the authtoken to save to configuration file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb-Cvq9O_Shj"
      },
      "source": [
        "!ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCLffuwO_g2w"
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEfOv6e6_zOm"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4vR2KrzA6KD",
        "outputId": "0dd4ce06-baf1-47c1-cccf-bcded6817943"
      },
      "source": [
        "!pgrep streamlit"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqOwDc65DASo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "441a135b-4151-49d7-93f2-be7f207f095d"
      },
      "source": [
        "!ngrok http\t./ngrok http 80"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME:\n",
            "   http - start an HTTP tunnel\n",
            "\n",
            "USAGE:\n",
            "   ngrok http [command options] [arguments...]\n",
            "\n",
            "DESCRIPTION:\n",
            "   Starts a tunnel listening for HTTP/HTTPS traffic with a specific hostname.\n",
            "   The HTTP Host header on incoming public requests is inspected to\n",
            "   determine which tunnel it matches.\n",
            "\n",
            "   HTTPS endpoints terminate TLS traffic at the ngrok server using the\n",
            "   ngrok.io certificates. The decrypted, HTTP traffic is then forwarded\n",
            "   through the secure tunnel and then to your local server. If you don't want\n",
            "   your TLS traffic to terminate at the ngrok server, use a TLS or TCP tunnel.\n",
            "\n",
            "EXAMPLES:\n",
            "   ngrok http 8080                      # forward ngrok.io subdomain to port 80\n",
            "   ngrok http example.com:9000          # forward traffic to example.com:9000\n",
            "   ngrok http -subdomain=bar 80         # request subdomain name: 'bar.ngrok.io'\n",
            "   ngrok http -hostname=ex.com 1234     # request tunnel 'ex.com' (DNS CNAME)\n",
            "   ngrok http -auth='falken:joshua' 80  # enforce basic auth on tunnel endpoint\n",
            "   ngrok http -host-header=ex.com 80    # rewrite the Host header to 'ex.com'\n",
            "   ngrok http file:///var/log           # serve local files in /var/log\n",
            "   ngrok http https://localhost:8443    # forward to a local https server\n",
            "\n",
            "OPTIONS:\n",
            "   --auth \t\tenforce basic auth on tunnel endpoint, 'user:password'\n",
            "   --authtoken \t\tngrok.com authtoken identifying a user\n",
            "   --bind-tls \"both\"\tlisten for http, https or both: true/false/both\n",
            "   --config\t\tpath to config files; they are merged if multiple\n",
            "   --host-header \tset Host header; if 'rewrite' use local address hostname\n",
            "   --hostname \t\thost tunnel on custom hostname (requires DNS CNAME)\n",
            "   --inspect\t\tenable/disable http introspection\n",
            "   --log \"false\"\tpath to log file, 'stdout', 'stderr' or 'false'\n",
            "   --log-format \"term\"\tlog record format: 'term', 'logfmt', 'json'\n",
            "   --log-level \"info\"\tlogging level\n",
            "   --region \t\tngrok server region [us, eu, au, ap, sa, jp, in] (default: us)\n",
            "   --subdomain \t\thost tunnel on a custom subdomain\n",
            "\n",
            "ERROR:  You must specify a single argument: a port or address to tunnel to.\n",
            "ERROR:  You specified 3 arguments: [./ngrok http 80]\n",
            "ERROR:  For example, to expose port 80, run 'ngrok http 80'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MrpuCfi9_tYM",
        "outputId": "0313745d-d4e2-4a5a-fa4e-6042301679a7"
      },
      "source": [
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://df89-35-196-191-248.ngrok.io'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXrqQQGrBlEH"
      },
      "source": [
        "!kill 265"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}