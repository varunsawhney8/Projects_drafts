{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Final Deploment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMaCij5rt4cG85J6hgky8gT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunsawhney8/Projects/blob/main/Fdeployedfinbert1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voR07Afz9g3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f50a768-a68b-4cf1-950a-ddb7f2e2ada9"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.7/dist-packages (0.87.0)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.17.3)\n",
            "Requirement already satisfied: validators in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.18.2)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: blinker in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: base58 in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.0)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.1.5)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.1.18)\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit) (21.2.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit) (3.7.4.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython!=3.1.19->streamlit) (4.0.7)\n",
            "Requirement already satisfied: smmap<5,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit) (4.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (6.3.1)\n",
            "Requirement already satisfied: ipython<8.0,>=7.23.1 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (7.27.0)\n",
            "Requirement already satisfied: importlib-metadata<5 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.6.4)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: argcomplete>=1.12.3 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (1.12.3)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<5->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.5.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (3.0.20)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (57.4.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (22.2.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8.0,>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.8.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.11.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2021.5.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWW_6v-fKVYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d94f5d18-6173-40a9-8e14-479c4106a09f"
      },
      "source": [
        "!pip install pyngrok==4.1.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (3.13)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15984 sha256=b87fc2aa9fc1702ea77f42fb2c960d0c2d13f33a8933fd3ba04e95b7d11cff86\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9mDL2XtMJll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbe384a-12b7-4ef0-8492-63e3d047c346"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 9.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsZCX-1K-jcE",
        "outputId": "15d38665-fecf-4b2d-e173-d6942e7933ba"
      },
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# Project: Sentiment Analysis \n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import nltk\n",
        "import requests\n",
        "import os\n",
        "import streamlit as st\n",
        "import altair as alt\n",
        "\n",
        "\n",
        "######## output ###############################################################\n",
        "\n",
        "#company_nse_2.to_csv(\"Reliance_Nse_data2.csv\",encoding = \"utf-8\")\n",
        "\n",
        "# b) Load dataset\n",
        "import pandas as pd\n",
        "path='/content/Reliance_Final_Ndtv_data4.xlsx'\n",
        "\n",
        "data=pd.read_excel(path)\n",
        "\n",
        "data=data[::-1].reset_index()\n",
        "data.drop(columns=['index'],axis=True,inplace=True)\n",
        "\n",
        "path='/content/Reliance_Nse_data2.csv'\n",
        "\n",
        "data1=pd.read_csv(path)\n",
        "\n",
        "# 2. Summarize Data\n",
        "# a) Descriptive statistics\n",
        "# b) Data visualizations\n",
        "\n",
        "data.drop(columns=['Media','Article','Keywords'],inplace=True,axis=1)\n",
        "\n",
        "# 3. Prepare Data\n",
        "# a) Data Cleaning\n",
        "# b) Feature Selection\n",
        "# c) Data Transforms\n",
        "def clean_data(data):\n",
        "    \n",
        "    list1=[]\n",
        "    list1=data['Title']\n",
        "    import re\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    \n",
        "    for i in range(len(list1)):               \n",
        "        tokens = list1[i].split()\n",
        "        # prepare regex for char filtering\n",
        "        re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "        # remove punctuation from each word\n",
        "        tokens = [re_punc.sub('', w) for w in tokens]\n",
        "        # remove remaining tokens that are not alphabetic\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "        # filter out stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        # filter out short tokens\n",
        "        tokens = [word for word in tokens if len(word) > 1]\n",
        "        list1[i] = ' '.join(tokens)\n",
        "    data['Title']=list1\n",
        "    return data\n",
        "\n",
        "    \n",
        "#del list1\n",
        "\n",
        "# 4. Evaluate Algorithms\n",
        "# a) Split-out validation dataset- train_test_split\n",
        "# b) Test options and evaluation metric- Confusion matrix, accuracy, precision,recall, f measure\n",
        "# c) Spot Check Algorithms- Unsupervised [Textblob, Vader, LM Dictionary,FINBERT], Supervised-[Bert]\n",
        "# d) Compare Algorithms\n",
        "\n",
        "# =============================================================================\n",
        "# \"\"\" MODEL:**\n",
        "# 1. FINBERT\n",
        "# \"\"\"\n",
        "# =============================================================================\n",
        "#!pip install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "def model_sentiment(data):\n",
        "    classifier = pipeline('sentiment-analysis', model=\"ProsusAI/finbert\")\n",
        "    list3=[]\n",
        "    for i in range(len(data['Title'])):\n",
        "        a=(classifier(data['Title'][i]))\n",
        "        a=a[0]\n",
        "        a=list(a.values())[0]\n",
        "        label_f={'positive':1,'negative':2,'neutral':0}\n",
        "        list3.append(label_f[a])\n",
        "    data['1day_sentiment']=list3\n",
        "    del list3\n",
        "    return (data)\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"#4. Business Problem Analysis\n",
        "#     A. Sentiment Meter\n",
        "#     B. Price Movement analysis based on Sentiment meter\n",
        "# \"\"\"\n",
        "# \n",
        "# \n",
        "# =============================================================================\n",
        "\n",
        "def business(data,data1):\n",
        "    # Groupby based on dates as several news articles are published on a particular day\n",
        "\n",
        "    aggregation_functions = {'Title': 'first', '1day_sentiment': 'max'}\n",
        "    data = data.groupby(data['Date']).aggregate(aggregation_functions)\n",
        "    data.reset_index( inplace=True)\n",
        "    data['Date']=pd.to_datetime(data['Date']).dt.date\n",
        "    data['1day_sentiment']=data['1day_sentiment'].astype('float')\n",
        "    \n",
        "    # Calculating Exponential Weighted average for 3,7,15 days sentiments\n",
        "\n",
        "    data['3day_sentiment'] = round(data['1day_sentiment'].ewm(span=3).mean())\n",
        "    data['7day_sentiment'] = round(data['1day_sentiment'].ewm(span=7).mean())\n",
        "    data['15day_sentiment'] = round(data['1day_sentiment'].ewm(span=15).mean())\n",
        "    data.head()\n",
        "    \n",
        "    # Converting date time format\n",
        "    data1['Date']=pd.to_datetime(data1['Date']).dt.date\n",
        "    \n",
        "    # Calculating percentage change w.r.t closing price\n",
        "\n",
        "    data1['perc_change1'] =data1['Close'].pct_change(periods=1)*100\n",
        "\n",
        "    data2=data.drop(columns=['Title'],axis=1)\n",
        "    \n",
        "    # Imputation of perc_change\n",
        "    data2[\"perc_change1\"]=[np.nan for i in range(len(data))]\n",
        "    \n",
        "    label={0: 'Neutral',1:'Positive',2:'Negative'}\n",
        "    data2['1day_sentiment']=[label[data2['1day_sentiment'][i]]for i in range(len(data2))]\n",
        "    data2['3day_sentiment']=[label[data2['3day_sentiment'][i]]for i in range(len(data2))]\n",
        "    data2['7day_sentiment']=[label[data2['7day_sentiment'][i]]for i in range(len(data2))]\n",
        "    data2['15day_sentiment']=[label[data2['15day_sentiment'][i]]for i in range(len(data2))]\n",
        "    \n",
        "    \n",
        "    for i in range(len(data2)):\n",
        "      for j in range(len(data1)):\n",
        "        if data['Date'][i]==data1['Date'][j]:\n",
        "          data2['perc_change1'][i]=data1['perc_change1'][j+1]\n",
        "      count=0\n",
        "      if type(data2['perc_change1'][i]) and pd.isna(data2['perc_change1'][i]):\n",
        "        for j in range(len(data1)):\n",
        "          if data2['Date'][i]>data1['Date'][j]:\n",
        "            count+=1\n",
        "        data2['perc_change1'][i]=data1['perc_change1'][count]\n",
        "    \n",
        "    data2['perc_change1'][0]=np.nan\n",
        "    \n",
        "    data2['Close']=[np.nan for i in range(len(data))]\n",
        "    \n",
        "    for i in range(len(data2)):\n",
        "      for j in range(len(data1)):\n",
        "        if data['Date'][i]==data1['Date'][j]:\n",
        "          data2['Close'][i]=data1['Close'][j+1]\n",
        "      count=0\n",
        "      if type(data2['Close'][i]) and pd.isna(data2['Close'][i]):\n",
        "        for j in range(len(data1)):\n",
        "          if data2['Date'][i]>data1['Date'][j]:\n",
        "            count+=1\n",
        "        data2['Close'][i]=data1['Close'][count]\n",
        "\n",
        "    # To Study how many investment oppurtunities were profitable, loss making\n",
        "    data4=pd.DataFrame(columns=[\"Description\",\"Total Opportunities\",\"Profit(%)\",\"Loss(%)\",\"No Profit No Loss(%)\"])\n",
        "    desc=[]\n",
        "    total1=[]\n",
        "    profit1=[]\n",
        "    loss1=[]\n",
        "    npnl1=[]\n",
        "    col=['1day_sentiment', '3day_sentiment', '7day_sentiment','15day_sentiment']\n",
        "    for j in col:\n",
        "      profit=0\n",
        "      loss=0\n",
        "      npnl=0\n",
        "      for i in range(len(data2)):\n",
        "          if data2[j][i]=='Negative' and  data2['perc_change1'][i]<0:\n",
        "            profit+=1\n",
        "          elif data2[j][i]=='Negative' and  data2['perc_change1'][i]>0:\n",
        "            loss+=1\n",
        "            \n",
        "          if data2[j][i]=='Positive' and  data2['perc_change1'][i]>0:\n",
        "            profit+=1\n",
        "          elif data2[j][i]=='Positive' and  data2['perc_change1'][i]<0:\n",
        "            loss+=1\n",
        "          if data2[j][i]=='Neutral':\n",
        "            npnl+=1\n",
        "      total= profit + loss +npnl\n",
        "      desc.append(j)\n",
        "      total1.append(total)\n",
        "      profit1.append(profit/total*100)\n",
        "      loss1.append(loss/total*100)\n",
        "      npnl1.append(npnl/total*100)\n",
        "    \n",
        "    data4[\"Description\"]=[\"Rolling 1 day-News Day Event\",\"Rolling 3 day-News Day Event\",\"Rolling 7 day-News Day Event \",\"Rolling 15 day-News Day Event\"]\n",
        "    data4[\"Total Opportunities\"]=total\n",
        "    data4['Profit(%)']=profit1\n",
        "    data4['Loss(%)']=loss1\n",
        "    data4['No Profit No Loss(%)']=npnl1\n",
        "    \n",
        "    data_1=data.drop(columns=[\"Title\"],axis=1)\n",
        "    data_f1=data_1.iloc[len(data_1)-1,]\n",
        "    label={0.0: 'Neutral',1.0:'Positive',2.0:'Negative'}\n",
        "    f_data=pd.DataFrame(columns=[\"Description\", \"Sentiment\"])\n",
        "    f_data[\"Description\"]=[\"Rolling 1 day-News Day Event\",\"Rolling 3 day-News Day Event\",\"Rolling 7 day-News Day Event \",\"Rolling 15 day-News Day Event\"]\n",
        "    f_data[\"Sentiment\"]=[label[data_f1[i]] for i in range(1,len(data_f1),1)]\n",
        "    \n",
        "    return f_data,data4,data_f1\n",
        "\n",
        "\n",
        "#A. Cleaning the data\n",
        "data=clean_data(data)\n",
        "\n",
        "#B. Predicting Sentiments\n",
        "data=model_sentiment(data)\n",
        "\n",
        "#C. Measuring Backtesting Investment Oppurtunities\n",
        "f_data,data4,data_f1= business(data, data1)\n",
        "\n",
        "# Streamlit Deployment Code\n",
        "\n",
        "companies=['1:INFOSYS LTD.', '2:TATA CONSULTANCY SERVICES LTD.' , '3:RELIANCE INDUSTRIES LTD.',\n",
        " '4:ICICI BANK LTD.', '5: HDFC BANK LTD.', '6: HCL TECHNOLOGIES LTD.',\n",
        " '7:BHARTI AIRTEL LTD.', '8: INDUSIND BANK LTD.', '9: STATE BANK OF INDIA',\n",
        "'10: LARSEN & TOUBRO LTD.', '11: TECH MAHINDRA LTD.', '12: AXIS BANK LTD.',\n",
        "'13: ITC LTD.', '14: BAJAJ AUTO LTD.', '15: OIL AND NATURAL GAS CORPORATION LTD.',\n",
        "'16: TATA STEEL LTD.', '17: NTPC LTD.', '18: MAHINDRA & MAHINDRA LTD.',\n",
        "'19: ASIAN PAINTS LTD.', '20: POWER GRID CORPORATION OF INDIA LTD.',\n",
        "'21: BAJAJ FINSERV LTD.', '22: TITAN COMPANY LTD.', '23: NESTLE INDIA LTD.',\n",
        "'24: ULTRATECH CEMENT LTD.' , '25: SUN PHARMACEUTICAL INDUSTRIES LTD.',\n",
        "'26:BAJAJ FINANCE LTD.', '27: MARUTI SUZUKI INDIA LTD.', '28: HOUSING DEVELOPMENT FINANCE CORP. LTD',\n",
        "'29: HINDUSTAN UNILEVER LTD.', '30: KOTAK MAHINDRA BANK LTD.']\n",
        "\n",
        "from PIL import Image\n",
        "st.set_page_config(page_title=\"NLP applications in Finance\", page_icon=None, layout='wide', initial_sidebar_state='expanded')\n",
        "image = Image.open('/content/IMG2.png')\n",
        "st.image(image, use_column_width=True)\n",
        "\n",
        "\n",
        "#st.sidebar.subheader(\"About us\")\n",
        "st.sidebar.title(\"About us\")\n",
        "st.sidebar.write('''\n",
        "Innodatatics is actively transformed by its highly extreme entities with various decades of realm \n",
        "expertise among its representatives. Our R&D team advances to conceive by collaborating with its alma maters \n",
        "in finding solution industry-complicated issues.''')\n",
        "\n",
        "st.sidebar.subheader(\"Project\")\n",
        "st.sidebar.write(\"Measuring Impact of Financial News on Stock Market\")\n",
        "select = st.sidebar.selectbox( \"Choose a company?\",(companies))\n",
        "st.sidebar.subheader(\"Contact us\")\n",
        "st.sidebar.write(\"Innodatatics Inc\")\n",
        "st.sidebar.write(\"[Website](https://innodatatics.ai)\")\n",
        "st.sidebar.write(\"© Copyrights 2021 Innodatatics\")\n",
        "\n",
        "\n",
        "\n",
        "#select = st.selectbox\n",
        "x=int(select[0])\n",
        "\n",
        "\n",
        "st.subheader(\"Selected Company:\")\n",
        "st.write(select[2:])\n",
        "\n",
        "# PIE CHART CODE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "labels = ['Neutral', 'Positive','Negative']\n",
        "sizes = [data['1day_sentiment'].value_counts()[0],data['1day_sentiment'].value_counts()[1],data['1day_sentiment'].value_counts()[2]]\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "#st.subheader('News Classification based on sentiment Analysis')\n",
        "ax.pie(sizes, labels=labels, autopct='%1.1f%%', shadow=False, startangle=140)\n",
        "plt.title('News type based on sentiment Analysis')\n",
        "from io import BytesIO\n",
        "buf1 = BytesIO()\n",
        "fig.savefig(buf1, format=\"png\")\n",
        "#st.image(buf1)\n",
        "\n",
        "# Word CLoud\n",
        "pos=[]\n",
        "neg=[]\n",
        "neutral=[]\n",
        "          \n",
        "# Data Divide into segments\n",
        "          \n",
        "for i in range(len(data)):\n",
        "    if data['1day_sentiment'][i]=='negative':\n",
        "        pos.append(data['Title'][i])\n",
        "    elif data['1day_sentiment'][i]=='positive':\n",
        "        neg.append(data['Title'][i])\n",
        "    else:\n",
        "        neutral.append(data['Title'][i])\n",
        "\n",
        "#             \n",
        "from collections import Counter\n",
        "vocab=Counter()          \n",
        "#             \n",
        "vocab_pos=[]\n",
        "#             \n",
        "for i in range(len(pos)):\n",
        "    tokens=pos[i].split()\n",
        "    for j in tokens:\n",
        "        vocab_pos.append(j)\n",
        "#             \n",
        "vocab_neg=[]\n",
        "#             \n",
        "for i in range(len(neg)):\n",
        "    tokens=neg[i].split()\n",
        "    for j in tokens:\n",
        "        vocab_neg.append(j)\n",
        "  #             \n",
        "vocab_neutral=[]\n",
        "#             \n",
        "for i in range(len(neutral)):\n",
        "    tokens=neutral[i].split()\n",
        "    for j in tokens:\n",
        "        vocab_neutral.append(j)\n",
        "#                          \n",
        "vocab.update(vocab_pos)\n",
        "vocab.update(vocab_neutral)\n",
        "vocab.update(vocab_neg)          \n",
        "vocab.most_common(50)\n",
        "#\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "#             \n",
        "other_stopwords_to_remove = ['\\\\n', 'n', '\\\\', '>', 'nLines', 'nI',\"n'\", \"hi\"]\n",
        "STOPWORDS = STOPWORDS.union(set(other_stopwords_to_remove))\n",
        "stopwords = set(STOPWORDS)\n",
        "text = str(vocab)\n",
        "wordcloud = WordCloud(width = 1800, height = 1800, background_color ='white', \n",
        "                          max_words=200, stopwords = stopwords, min_font_size = 10).generate(text)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4,4))\n",
        "ax.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.title(\"News based word cloud\")\n",
        "plt.axis(\"off\")\n",
        "from io import BytesIO\n",
        "buf2 = BytesIO()\n",
        "fig.savefig(buf2, format=\"png\")\n",
        "#st.image(buf2)\n",
        "\n",
        "images = [buf1,buf2]\n",
        "st.image(images, use_column_width=False)\n",
        "\n",
        "st.subheader(\"Current News Based Sentiment Report\")\n",
        "st.write(\"Last News Day Recoded : \",data_f1[0])\n",
        "#f_data.set_index('Description')\n",
        "st.write(f_data)\n",
        "st.subheader(\"Back Testing News Based Investment Oppurtunities\")\n",
        "st.subheader(\"Square off trade within  next trading day \")\n",
        "st.write(data4)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ry1HB8xmkwt3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji6Jkycu-2_H",
        "outputId": "250dcd71-cf75-45fd-fcac-cd6afbbca541"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\t  Reliance_Final_Ndtv_data4.xlsx  sample_data\n",
            "nlp2.PNG  Reliance_Nse_data2.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWhL5XRR_EM5",
        "outputId": "ba394f79-f6a4-4e3f-e97a-5be139c35624"
      },
      "source": [
        "!ngrok authtoken ./ngrok authtoken 1xHDQ0ZVGhTEWf2WvFJWvwgCW9U_7MWDgdeCydvLzgnNvBSa4"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME:\n",
            "   authtoken - save authtoken to configuration file\n",
            "\n",
            "USAGE:\n",
            "   ngrok authtoken [command options] [arguments...]\n",
            "\n",
            "DESCRIPTION:\n",
            "   The authtoken command modifies your configuration file to include\n",
            "   the specified authtoken. By default, this configuration file is located\n",
            "   at $HOME/.ngrok2/ngrok.yml\n",
            "\n",
            "   The ngrok.com service requires that you sign up for an account to use\n",
            "   many advanced service features. In order to associate your client with\n",
            "   an account, it must pass a secret token to the ngrok.com service when it\n",
            "   starts up. Instead of passing this authtoken on every invocation, you may\n",
            "   use this command to save it into your configuration file so that your\n",
            "   client always authenticates you properly.\n",
            "\n",
            "EXAMPLE:\n",
            "    ngrok authtoken BDZIXnhJt2HNWLXyQ5PM_qCaBq0W2sNFcCa0rfTZd\n",
            "\n",
            "OPTIONS:\n",
            "   --config \t\tsave in this config file, default: ~/.ngrok2/ngrok.yml\n",
            "   --log \"false\"\tpath to log file, 'stdout', 'stderr' or 'false'\n",
            "   --log-format \"term\"\tlog record format: 'term', 'logfmt', 'json'\n",
            "   --log-level \"info\"\tlogging level\n",
            "\n",
            "ERROR:  You must pass a single argument, the authtoken to save to configuration file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb-Cvq9O_Shj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bef07e9-e99b-43bb-f56c-6b1dec1c9004"
      },
      "source": [
        "!ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME:\n",
            "   ngrok - tunnel local ports to public URLs and inspect traffic\n",
            "\n",
            "DESCRIPTION:\n",
            "    ngrok exposes local networked services behinds NATs and firewalls to the\n",
            "    public internet over a secure tunnel. Share local websites, build/test\n",
            "    webhook consumers and self-host personal services.\n",
            "    Detailed help for each command is available with 'ngrok help <command>'.\n",
            "    Open http://localhost:4040 for ngrok's web interface to inspect traffic.\n",
            "\n",
            "EXAMPLES:\n",
            "    ngrok http 80                    # secure public URL for port 80 web server\n",
            "    ngrok http -subdomain=baz 8080   # port 8080 available at baz.ngrok.io\n",
            "    ngrok http foo.dev:80            # tunnel to host:port instead of localhost\n",
            "    ngrok http https://localhost     # expose a local https server\n",
            "    ngrok tcp 22                     # tunnel arbitrary TCP traffic to port 22\n",
            "    ngrok tls -hostname=foo.com 443  # TLS traffic for foo.com to port 443\n",
            "    ngrok start foo bar baz          # start tunnels from the configuration file\n",
            "\n",
            "VERSION:\n",
            "   2.3.40\n",
            "\n",
            "AUTHOR:\n",
            "  inconshreveable - <alan@ngrok.com>\n",
            "\n",
            "COMMANDS:\n",
            "   authtoken\tsave authtoken to configuration file\n",
            "   credits\tprints author and licensing information\n",
            "   http\t\tstart an HTTP tunnel\n",
            "   start\tstart tunnels by name from the configuration file\n",
            "   tcp\t\tstart a TCP tunnel\n",
            "   tls\t\tstart a TLS tunnel\n",
            "   update\tupdate ngrok to the latest version\n",
            "   version\tprint the version string\n",
            "   help\t\tShows a list of commands or help for one command\n",
            "\n",
            "PYNGROK VERSION:\n",
            "   4.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCLffuwO_g2w"
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEfOv6e6_zOm"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4vR2KrzA6KD",
        "outputId": "2d8ccac0-6d6a-4525-f2cb-8a1acdb094b2"
      },
      "source": [
        "!pgrep streamlit"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqOwDc65DASo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46146e1c-7048-4f15-d846-9b9eb2d1e421"
      },
      "source": [
        "!ngrok http\t./ngrok http 8501"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME:\n",
            "   http - start an HTTP tunnel\n",
            "\n",
            "USAGE:\n",
            "   ngrok http [command options] [arguments...]\n",
            "\n",
            "DESCRIPTION:\n",
            "   Starts a tunnel listening for HTTP/HTTPS traffic with a specific hostname.\n",
            "   The HTTP Host header on incoming public requests is inspected to\n",
            "   determine which tunnel it matches.\n",
            "\n",
            "   HTTPS endpoints terminate TLS traffic at the ngrok server using the\n",
            "   ngrok.io certificates. The decrypted, HTTP traffic is then forwarded\n",
            "   through the secure tunnel and then to your local server. If you don't want\n",
            "   your TLS traffic to terminate at the ngrok server, use a TLS or TCP tunnel.\n",
            "\n",
            "EXAMPLES:\n",
            "   ngrok http 8080                      # forward ngrok.io subdomain to port 80\n",
            "   ngrok http example.com:9000          # forward traffic to example.com:9000\n",
            "   ngrok http -subdomain=bar 80         # request subdomain name: 'bar.ngrok.io'\n",
            "   ngrok http -hostname=ex.com 1234     # request tunnel 'ex.com' (DNS CNAME)\n",
            "   ngrok http -auth='falken:joshua' 80  # enforce basic auth on tunnel endpoint\n",
            "   ngrok http -host-header=ex.com 80    # rewrite the Host header to 'ex.com'\n",
            "   ngrok http file:///var/log           # serve local files in /var/log\n",
            "   ngrok http https://localhost:8443    # forward to a local https server\n",
            "\n",
            "OPTIONS:\n",
            "   --auth \t\tenforce basic auth on tunnel endpoint, 'user:password'\n",
            "   --authtoken \t\tngrok.com authtoken identifying a user\n",
            "   --bind-tls \"both\"\tlisten for http, https or both: true/false/both\n",
            "   --config\t\tpath to config files; they are merged if multiple\n",
            "   --host-header \tset Host header; if 'rewrite' use local address hostname\n",
            "   --hostname \t\thost tunnel on custom hostname (requires DNS CNAME)\n",
            "   --inspect\t\tenable/disable http introspection\n",
            "   --log \"false\"\tpath to log file, 'stdout', 'stderr' or 'false'\n",
            "   --log-format \"term\"\tlog record format: 'term', 'logfmt', 'json'\n",
            "   --log-level \"info\"\tlogging level\n",
            "   --region \t\tngrok server region [us, eu, au, ap, sa, jp, in] (default: us)\n",
            "   --subdomain \t\thost tunnel on a custom subdomain\n",
            "\n",
            "ERROR:  You must specify a single argument: a port or address to tunnel to.\n",
            "ERROR:  You specified 3 arguments: [./ngrok http 8501]\n",
            "ERROR:  For example, to expose port 80, run 'ngrok http 80'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MrpuCfi9_tYM",
        "outputId": "c34f6cf0-525a-48c4-cf4a-92bce64d3a27"
      },
      "source": [
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://7508-34-83-103-135.ngrok.io'"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXrqQQGrBlEH"
      },
      "source": [
        "!kill 265"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}