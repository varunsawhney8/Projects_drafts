{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "The Final Deploment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOQ6p0VWkgmpLmBkAya15Sn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/varunsawhney8/Projects/blob/main/Fdeployedfinbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voR07Afz9g3x"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWW_6v-fKVYZ"
      },
      "source": [
        "!pip install pyngrok==4.1.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9mDL2XtMJll"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsZCX-1K-jcE",
        "outputId": "e5961ea8-540a-466b-d74c-76eb60fc064a"
      },
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# Project: Sentiment Analysis \n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import nltk\n",
        "import requests\n",
        "import os\n",
        "import streamlit as st\n",
        "import altair as alt\n",
        "\n",
        "\n",
        "######## output ###############################################################\n",
        "\n",
        "#company_nse_2.to_csv(\"Reliance_Nse_data2.csv\",encoding = \"utf-8\")\n",
        "\n",
        "# b) Load dataset\n",
        "import pandas as pd\n",
        "path='/content/Reliance_Final_Ndtv_data4.xlsx'\n",
        "\n",
        "data=pd.read_excel(path)\n",
        "\n",
        "data=data[::-1].reset_index()\n",
        "data.drop(columns=['index'],axis=True,inplace=True)\n",
        "\n",
        "path='/content/Reliance_Nse_data2.csv'\n",
        "\n",
        "data1=pd.read_csv(path)\n",
        "\n",
        "# 2. Summarize Data\n",
        "# a) Descriptive statistics\n",
        "# b) Data visualizations\n",
        "\n",
        "data.drop(columns=['Media','Article','Keywords'],inplace=True,axis=1)\n",
        "\n",
        "# 3. Prepare Data\n",
        "# a) Data Cleaning\n",
        "# b) Feature Selection\n",
        "# c) Data Transforms\n",
        "def clean_data(data):\n",
        "    \n",
        "    list1=[]\n",
        "    list1=data['Title']\n",
        "    import re\n",
        "    import nltk\n",
        "    nltk.download('stopwords')\n",
        "    \n",
        "    for i in range(len(list1)):               \n",
        "        tokens = list1[i].split()\n",
        "        # prepare regex for char filtering\n",
        "        re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "        # remove punctuation from each word\n",
        "        tokens = [re_punc.sub('', w) for w in tokens]\n",
        "        # remove remaining tokens that are not alphabetic\n",
        "        tokens = [word for word in tokens if word.isalpha()]\n",
        "        # filter out stop words\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [w for w in tokens if not w in stop_words]\n",
        "        # filter out short tokens\n",
        "        tokens = [word for word in tokens if len(word) > 1]\n",
        "        list1[i] = ' '.join(tokens)\n",
        "    data['Title']=list1\n",
        "    return data\n",
        "\n",
        "    \n",
        "#del list1\n",
        "\n",
        "# 4. Evaluate Algorithms\n",
        "# a) Split-out validation dataset- train_test_split\n",
        "# b) Test options and evaluation metric- Confusion matrix, accuracy, precision,recall, f measure\n",
        "# c) Spot Check Algorithms- Unsupervised [Textblob, Vader, LM Dictionary,FINBERT], Supervised-[Bert]\n",
        "# d) Compare Algorithms\n",
        "\n",
        "# =============================================================================\n",
        "# \"\"\" MODEL:**\n",
        "# 1. FINBERT\n",
        "# \"\"\"\n",
        "# =============================================================================\n",
        "#!pip install transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "def model_sentiment(data):\n",
        "    \n",
        "    classifier = pipeline('sentiment-analysis', model=\"ProsusAI/finbert\")\n",
        "    list3=[]\n",
        "    \n",
        "    for i in range(len(data['Title'])):\n",
        "        a=(classifier(data['Title'][i]))\n",
        "        a=a[0]\n",
        "        a=list(a.values())[0]\n",
        "        label_f={'positive':1,'negative':2,'neutral':0}\n",
        "        list3.append(label_f[a])\n",
        "    \n",
        "    data['1day_sentiment']=list3\n",
        "    del list3\n",
        "    return (data)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# \n",
        "# \"\"\"**VISUALIZATION:**\n",
        "# 1. PIE Chart\n",
        "# 2. WORD Cloud\n",
        "# \"\"\"\n",
        "# def visualizations(data):\n",
        "#     \n",
        "#             pos=[]\n",
        "#             neg=[]\n",
        "#             neutral=[]\n",
        "#             \n",
        "#             # Data Divide into segments\n",
        "#             \n",
        "#             for i in range(len(data)):\n",
        "#                 if data['1day_sentiment'][i]=='negative':\n",
        "#                     pos.append(data['Title'][i])\n",
        "#                 elif data['1day_sentiment'][i]=='positive':\n",
        "#                     neg.append(data['Title'][i])\n",
        "#                 else:\n",
        "#                     neutral.append(data['Title'][i])\n",
        "#             \n",
        "#             \n",
        "#             \n",
        "#             import matplotlib.pyplot as plt\n",
        "#             labels = ['Neutral', 'Positive','Negative']\n",
        "#             sizes = [data['1day_sentiment'].value_counts()[0],data['1day_sentiment'].value_counts()[1],data['1day_sentiment'].value_counts()[2]]\n",
        "#             # Plot\n",
        "#             plt.title('News Classification based on sentiment Analysis')\n",
        "#             plt.pie(sizes, labels=labels, \n",
        "#                     autopct='%1.1f%%', shadow=True, startangle=140)\n",
        "#             plt.axis('equal')\n",
        "#             plt.show()\n",
        "#             \n",
        "#             from collections import Counter\n",
        "#             vocab=Counter()\n",
        "#             \n",
        "#             \n",
        "#             vocab_pos=[]\n",
        "#             \n",
        "#             for i in range(len(pos)):\n",
        "#                 tokens=pos[i].split()\n",
        "#                 for j in tokens:\n",
        "#                     vocab_pos.append(j)\n",
        "#             \n",
        "#             vocab_neg=[]\n",
        "#             \n",
        "#             for i in range(len(neg)):\n",
        "#                 tokens=neg[i].split()\n",
        "#                 for j in tokens:\n",
        "#                     vocab_neg.append(j)\n",
        "#             \n",
        "#             vocab_neutral=[]\n",
        "#             \n",
        "#             for i in range(len(neutral)):\n",
        "#                 tokens=neutral[i].split()\n",
        "#                 for j in tokens:\n",
        "#                     vocab_neutral.append(j)\n",
        "#             \n",
        "#             \n",
        "#             vocab.update(vocab_pos)\n",
        "#             vocab.update(vocab_neutral)\n",
        "#             \n",
        "#             vocab.update(vocab_neg)\n",
        "#             \n",
        "#             \n",
        "#             vocab.most_common(50)\n",
        "#             \n",
        "#             from wordcloud import WordCloud, STOPWORDS\n",
        "#             import matplotlib.pyplot as plt\n",
        "#             \n",
        "#             other_stopwords_to_remove = ['\\\\n', 'n', '\\\\', '>', 'nLines', 'nI',\"n'\", \"hi\"]\n",
        "#             STOPWORDS = STOPWORDS.union(set(other_stopwords_to_remove))\n",
        "#             stopwords = set(STOPWORDS)\n",
        "#             text = str(vocab)\n",
        "#             wordcloud = WordCloud(width = 1800, height = 1800, \n",
        "#                             background_color ='white', \n",
        "#                             max_words=200,\n",
        "#                             stopwords = stopwords, \n",
        "#                             min_font_size = 10).generate(text)\n",
        "#             plt.imshow(wordcloud, interpolation='bilinear')\n",
        "#             plt.axis(\"off\")\n",
        "#             plt.show()\n",
        "\n",
        "# \"\"\"#5. Business Problem Analysis\n",
        "#     A. Sentiment Meter\n",
        "#     B. Price Movement analysis based on Sentiment meter\n",
        "# \"\"\"\n",
        "# \n",
        "# \n",
        "# =============================================================================\n",
        "\n",
        "def business(data,data1):\n",
        "    # Groupby based on dates as several news articles are published on a particular day\n",
        "\n",
        "    aggregation_functions = {'Title': 'first', '1day_sentiment': 'max'}\n",
        "    data = data.groupby(data['Date']).aggregate(aggregation_functions)\n",
        "    data.reset_index( inplace=True)\n",
        "    data['Date']=pd.to_datetime(data['Date']).dt.date\n",
        "    data['1day_sentiment']=data['1day_sentiment'].astype('float')\n",
        "    \n",
        "    # Calculating Exponential Weighted average for 3,7,15 days sentiments\n",
        "\n",
        "    data['3day_sentiment'] = round(data['1day_sentiment'].ewm(span=3).mean())\n",
        "    data['7day_sentiment'] = round(data['1day_sentiment'].ewm(span=7).mean())\n",
        "    data['15day_sentiment'] = round(data['1day_sentiment'].ewm(span=15).mean())\n",
        "    data.head()\n",
        "    \n",
        "    # Converting date time format\n",
        "    data1['Date']=pd.to_datetime(data1['Date']).dt.date\n",
        "    \n",
        "    # Calculating percentage change w.r.t closing price\n",
        "\n",
        "    data1['perc_change1'] =data1['Close'].pct_change(periods=1)*100\n",
        "    \n",
        "    #data1['perc_change3'] =data1['Close'].pct_change(periods=2)*100\n",
        "    #data1['perc_change7'] =data1['Close'].pct_change(periods=6)*100\n",
        "        \n",
        "    #--\n",
        "    #----\n",
        "    data2=data.drop(columns=['Title'],axis=1)\n",
        "    \n",
        "    # Imputation of perc_change\n",
        "\n",
        "    data2[\"perc_change1\"]=[np.nan for i in range(len(data))]\n",
        "    \n",
        "    label={0: 'Neutral',1:'Positive',2:'Negative'}\n",
        "    data2['1day_sentiment']=[label[data2['1day_sentiment'][i]]for i in range(len(data2))]\n",
        "    data2['3day_sentiment']=[label[data2['3day_sentiment'][i]]for i in range(len(data2))]\n",
        "    data2['7day_sentiment']=[label[data2['7day_sentiment'][i]]for i in range(len(data2))]\n",
        "    data2['15day_sentiment']=[label[data2['15day_sentiment'][i]]for i in range(len(data2))]\n",
        "    \n",
        "    \n",
        "    for i in range(len(data2)):\n",
        "      for j in range(len(data1)):\n",
        "        if data['Date'][i]==data1['Date'][j]:\n",
        "          data2['perc_change1'][i]=data1['perc_change1'][j+1]\n",
        "      count=0\n",
        "      if type(data2['perc_change1'][i]) and pd.isna(data2['perc_change1'][i]):\n",
        "        for j in range(len(data1)):\n",
        "          if data2['Date'][i]>data1['Date'][j]:\n",
        "            count+=1\n",
        "        data2['perc_change1'][i]=data1['perc_change1'][count]\n",
        "    \n",
        "    data2['perc_change1'][0]=np.nan\n",
        "    \n",
        "    data2['Close']=[np.nan for i in range(len(data))]\n",
        "    \n",
        "    for i in range(len(data2)):\n",
        "      for j in range(len(data1)):\n",
        "        if data['Date'][i]==data1['Date'][j]:\n",
        "          data2['Close'][i]=data1['Close'][j+1]\n",
        "      count=0\n",
        "      if type(data2['Close'][i]) and pd.isna(data2['Close'][i]):\n",
        "        for j in range(len(data1)):\n",
        "          if data2['Date'][i]>data1['Date'][j]:\n",
        "            count+=1\n",
        "        data2['Close'][i]=data1['Close'][count]\n",
        "\n",
        "    # To Study how many investment oppurtunities were profitable, loss making\n",
        "    data4=pd.DataFrame(columns=[\"Description\",\"Total Opportunities\",\"Profit(%)\",\"Loss(%)\",\"No Profit No Loss(%)\"])\n",
        "    desc=[]\n",
        "    total1=[]\n",
        "    profit1=[]\n",
        "    loss1=[]\n",
        "    npnl1=[]\n",
        "    col=['1day_sentiment', '3day_sentiment', '7day_sentiment','15day_sentiment']\n",
        "    for j in col:\n",
        "      profit=0\n",
        "      loss=0\n",
        "      npnl=0\n",
        "      for i in range(len(data2)):\n",
        "          if data2[j][i]=='Negative' and  data2['perc_change1'][i]<0:\n",
        "            profit+=1\n",
        "          elif data2[j][i]=='Negative' and  data2['perc_change1'][i]>0:\n",
        "            loss+=1\n",
        "            \n",
        "          if data2[j][i]=='Positive' and  data2['perc_change1'][i]>0:\n",
        "            profit+=1\n",
        "          elif data2[j][i]=='Positive' and  data2['perc_change1'][i]<0:\n",
        "            loss+=1\n",
        "          if data2[j][i]=='Neutral':\n",
        "            npnl+=1\n",
        "      total= profit + loss +npnl\n",
        "      desc.append(j)\n",
        "      total1.append(total)\n",
        "      profit1.append(profit/total*100)\n",
        "      loss1.append(loss/total*100)\n",
        "      npnl1.append(npnl/total*100)\n",
        "    \n",
        "    data4[\"Description\"]=[\"Rolling 1 day-News Day Event\",\"Rolling 3 day-News Day Event\",\"Rolling 7 day-News Day Event \",\"Rolling 15 day-News Day Event\"]\n",
        "    data4[\"Total Opportunities\"]=total\n",
        "    data4['Profit(%)']=profit1\n",
        "    data4['Loss(%)']=loss1\n",
        "    data4['No Profit No Loss(%)']=npnl1\n",
        "    print(\"Back Testing News Based Investment Oppurtunities\")\n",
        "    print(\"Square off trade within  next trading day \")\n",
        "    #display(data4)\n",
        "    \n",
        "    data_1=data.drop(columns=[\"Title\"],axis=1)\n",
        "    data_f1=data_1.iloc[len(data_1)-1,]\n",
        "    label={0.0: 'Neutral',1.0:'Positive',2.0:'Negative'}\n",
        "    f_data=pd.DataFrame(columns=[\"Description\", \"Sentiment\"])\n",
        "    f_data[\"Description\"]=[\"Rolling 1 day-News Day Event\",\"Rolling 3 day-News Day Event\",\"Rolling 7 day-News Day Event \",\"Rolling 15 day-News Day Event\"]\n",
        "    f_data[\"Sentiment\"]=[label[data_f1[i]] for i in range(1,len(data_f1),1)]\n",
        "    \n",
        "    print(\"Current News Based Sentiment Report\")\n",
        "    print(\"Last News Day Recoded : \",data_f1[0])\n",
        "    del data_1,data_f1\n",
        "    #from IPython.display import display\n",
        "    #display(f_data)\n",
        "    return f_data,data4\n",
        "\n",
        "\n",
        "# 5. Improve Accuracy\n",
        "# a) Algorithm Tuning\n",
        "# b) Ensembles\n",
        "\n",
        "# 6. Finalize Model\n",
        "# a) Predictions on validation dataset\n",
        "# b) Create standalone model on entire training dataset\n",
        "# c) Save model for later use\n",
        "\n",
        "\n",
        "data=clean_data(data)\n",
        "data=model_sentiment(data)\n",
        "f_data,data4= business(data, data1)\n",
        "st.write(f_data,data4)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji6Jkycu-2_H",
        "outputId": "c3fd23ef-6e88-4a30-bea3-21bb93c2069b"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "app.py\tReliance_Final_Ndtv_data4.xlsx\tReliance_Nse_data2.csv\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWhL5XRR_EM5",
        "outputId": "98f47f60-3df4-472a-8a0d-8100ae6ebece"
      },
      "source": [
        "!ngrok authtoken ./ngrok authtoken 1xHDQ0ZVGhTEWf2WvFJWvwgCW9U_7MWDgdeCydvLzgnNvBSa4"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NAME:\n",
            "   authtoken - save authtoken to configuration file\n",
            "\n",
            "USAGE:\n",
            "   ngrok authtoken [command options] [arguments...]\n",
            "\n",
            "DESCRIPTION:\n",
            "   The authtoken command modifies your configuration file to include\n",
            "   the specified authtoken. By default, this configuration file is located\n",
            "   at $HOME/.ngrok2/ngrok.yml\n",
            "\n",
            "   The ngrok.com service requires that you sign up for an account to use\n",
            "   many advanced service features. In order to associate your client with\n",
            "   an account, it must pass a secret token to the ngrok.com service when it\n",
            "   starts up. Instead of passing this authtoken on every invocation, you may\n",
            "   use this command to save it into your configuration file so that your\n",
            "   client always authenticates you properly.\n",
            "\n",
            "EXAMPLE:\n",
            "    ngrok authtoken BDZIXnhJt2HNWLXyQ5PM_qCaBq0W2sNFcCa0rfTZd\n",
            "\n",
            "OPTIONS:\n",
            "   --config \t\tsave in this config file, default: ~/.ngrok2/ngrok.yml\n",
            "   --log \"false\"\tpath to log file, 'stdout', 'stderr' or 'false'\n",
            "   --log-format \"term\"\tlog record format: 'term', 'logfmt', 'json'\n",
            "   --log-level \"info\"\tlogging level\n",
            "\n",
            "ERROR:  You must pass a single argument, the authtoken to save to configuration file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb-Cvq9O_Shj"
      },
      "source": [
        "!ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCLffuwO_g2w"
      },
      "source": [
        "from pyngrok import ngrok"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEfOv6e6_zOm"
      },
      "source": [
        "!streamlit run app.py &>/dev/null&"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4vR2KrzA6KD",
        "outputId": "2118670b-b813-4efa-d510-a709f34565a2"
      },
      "source": [
        "!pgrep streamlit"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqOwDc65DASo"
      },
      "source": [
        "!ngrok http\t./ngrok http 80"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MrpuCfi9_tYM",
        "outputId": "6ea5a9e0-9bc4-4ece-a8eb-da2f78a6bc2f"
      },
      "source": [
        "public_url = ngrok.connect(port='8501')\n",
        "public_url"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'http://abf0-34-82-68-2.ngrok.io'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXrqQQGrBlEH"
      },
      "source": [
        "!kill 228"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}